#include rob
#include nehalem
[perf_model/core]
frequency = 2.66
type = rob

[perf_model/core/interval_timer]
dispatch_width = 2
window_size = 256

[perf_model/core/rob_timer]
outstanding_loads = 32
outstanding_stores = 16
commit_width = 64
rs_entries = 32

[perf_model/l1_icache]
cache_size = 8
associativity = 4
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l1_dcache]
perfect = false
cache_size = 4
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l2_cache]
perfect = false
cache_size = 256
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 8 # 8.something according to membench, -1 cycle L1 tag access time
tags_access_time = 3
writeback_time = 50 # L3 hit time will be added
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l3_cache]
perfect = false
cache_block_size = 64
cache_size = 8192
associativity = 16
address_hash = mask
replacement_policy = lru
data_access_time = 30 # 35 cycles total according to membench, +L1+L2 tag times
tags_access_time = 10
perf_model_type = parallel
writethrough = 0
shared_cores = 4

[perf_model/dram_directory]
total_entries = 1048576
associativity = 16
directory_type = full_map

[perf_model/dram]
num_controllers = -1
controllers_interleaving = 4
latency = 45
per_controller_bandwidth = 7.6              # In GB/s, as measured by core_validation-dram
chips_per_dimm = 8
dimms_per_controller = 4

[network]
memory_model_1 = bus
memory_model_2 = bus

[network/bus]
bandwidth = 25.6 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links

[power]
technology_node = 22 # nm

